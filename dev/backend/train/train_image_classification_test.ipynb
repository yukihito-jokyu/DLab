{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 122880000 into shape (1,28,28)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 219\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;66;03m# 実行\u001b[39;00m\n\u001b[0;32m    204\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexample_user_id\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProject_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCIFAR-10\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    216\u001b[0m     }\n\u001b[0;32m    217\u001b[0m }\n\u001b[1;32m--> 219\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 108\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_model\u001b[39m(config):\n\u001b[0;32m    106\u001b[0m     model \u001b[38;5;241m=\u001b[39m import_model(config)\n\u001b[1;32m--> 108\u001b[0m     (x_train, y_train), (x_val, y_val), (x_test, y_test) \u001b[38;5;241m=\u001b[39m \u001b[43mload_and_split_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    110\u001b[0m     train_info \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain_info\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    112\u001b[0m     train_dataset \u001b[38;5;241m=\u001b[39m TensorDataset(torch\u001b[38;5;241m.\u001b[39mfrom_numpy(x_train)\u001b[38;5;241m.\u001b[39mfloat(), torch\u001b[38;5;241m.\u001b[39mfrom_numpy(y_train)\u001b[38;5;241m.\u001b[39mlong())\n",
      "Cell \u001b[1;32mIn[8], line 65\u001b[0m, in \u001b[0;36mload_and_split_data\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m     62\u001b[0m x_train, x_val, y_train, y_val \u001b[38;5;241m=\u001b[39m train_test_split(x_train, y_train, test_size\u001b[38;5;241m=\u001b[39mtest_size)\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# 形状を (batch_size, 1, 28, 28) に変換\u001b[39;00m\n\u001b[1;32m---> 65\u001b[0m x_train \u001b[38;5;241m=\u001b[39m \u001b[43mx_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m28\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m28\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m x_val \u001b[38;5;241m=\u001b[39m x_val\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m28\u001b[39m)\n\u001b[0;32m     67\u001b[0m x_test \u001b[38;5;241m=\u001b[39m x_test\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m28\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 122880000 into shape (1,28,28)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import importlib.util\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# GCN関数\n",
    "def global_contrast_normalization(X, scale=1.0, min_divisor=1e-8):\n",
    "    mean = X.mean(axis=1, keepdims=True)\n",
    "    X = X - mean\n",
    "    contrast = np.sqrt((X**2).sum(axis=1, keepdims=True))\n",
    "    contrast[contrast < min_divisor] = min_divisor\n",
    "    X = scale * X / contrast\n",
    "    return X\n",
    "\n",
    "# ZCA Whitening関数\n",
    "def zca_whitening(X):\n",
    "    X = X.reshape(X.shape[0], -1)\n",
    "    sigma = np.dot(X.T, X) / X.shape[0]\n",
    "    U, S, V = np.linalg.svd(sigma)\n",
    "    epsilon = 1e-5\n",
    "    ZCAMatrix = np.dot(U, np.dot(np.diag(1.0 / np.sqrt(S + epsilon)), U.T))\n",
    "    X = np.dot(X, ZCAMatrix)\n",
    "    return X.reshape(-1, 28, 28)\n",
    "\n",
    "# データセットを読み込む関数\n",
    "def import_model(config):\n",
    "    user_id = config[\"user_id\"]\n",
    "    project_name = config[\"Project_name\"]\n",
    "    model_id = config[\"model_id\"]\n",
    "    \n",
    "    base_dir = os.path.abspath(os.path.join(os.getcwd(), \"../user\"))\n",
    "    model_path = os.path.join(base_dir, user_id, project_name, model_id, \"model_config.py\")\n",
    "    spec = importlib.util.spec_from_file_location(\"Simple_NN\", model_path)\n",
    "    model_module = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(model_module)\n",
    "    return model_module.Simple_NN()\n",
    "\n",
    "# データセットの読込み＆前処理を行う関数\n",
    "def load_and_split_data(config):\n",
    "    project_name = config[\"Project_name\"]\n",
    "    dataset_dir = os.path.abspath(os.path.join(os.getcwd(), \"../dataset\", project_name))\n",
    "    \n",
    "    x_train = np.load(os.path.join(dataset_dir, \"x_train.npy\"))\n",
    "    y_train = np.load(os.path.join(dataset_dir, \"y_train.npy\"))\n",
    "    x_test = np.load(os.path.join(dataset_dir, \"x_test.npy\"))\n",
    "    y_test = np.load(os.path.join(dataset_dir, \"y_test.npy\"))\n",
    "\n",
    "    pretreatment = config[\"Train_info\"].get(\"Pretreatment\", \"none\")\n",
    "    if pretreatment == \"GCN\":\n",
    "        x_train = global_contrast_normalization(x_train)\n",
    "        x_test = global_contrast_normalization(x_test)\n",
    "    elif pretreatment == \"ZCA\":\n",
    "        x_train = zca_whitening(x_train)\n",
    "        x_test = zca_whitening(x_test)\n",
    "    \n",
    "    test_size = config[\"Train_info\"][\"test_size\"]\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=test_size)\n",
    "    \n",
    "    # 形状を (batch_size, 1, 28, 28) に変換\n",
    "    x_train = x_train.reshape(-1, 1, 28, 28)\n",
    "    x_val = x_val.reshape(-1, 1, 28, 28)\n",
    "    x_test = x_test.reshape(-1, 1, 28, 28)\n",
    "    \n",
    "    return (x_train, y_train), (x_val, y_val), (x_test, y_test)\n",
    "\n",
    "def get_optimizer(optimizer_name, params, lr):\n",
    "    if optimizer_name == 'SGD':\n",
    "        return optim.SGD(params, lr)\n",
    "    if optimizer_name == 'momentum':\n",
    "        return optim.SGD(params, lr, momentum=0.8)\n",
    "    if optimizer_name == 'Adam':\n",
    "        return optim.Adam(params, lr)\n",
    "    if optimizer_name == 'Adagrad':\n",
    "        return optim.Adagrad(params, lr)\n",
    "    if optimizer_name == 'RMSProp':\n",
    "        return optim.RMSprop(params, lr)\n",
    "    if optimizer_name == 'Adadelta':\n",
    "        return optim.Adadelta(params, lr)\n",
    "\n",
    "def get_loss(loss_name):\n",
    "    if loss_name == 'mse_loss':\n",
    "        return nn.MSELoss()\n",
    "    if loss_name == 'cross_entropy':\n",
    "        return nn.CrossEntropyLoss()\n",
    "    if loss_name == 'binary_cross_entropy':\n",
    "        return nn.BCELoss()\n",
    "    if loss_name == 'nll_loss':\n",
    "        return nn.NLLLoss()\n",
    "    if loss_name == 'hinge_embedding_loss':\n",
    "        return nn.HingeEmbeddingLoss()\n",
    "\n",
    "# 正答率を算出する関数\n",
    "def calculate_accuracy(outputs, targets):\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    corrects = (preds == targets).sum().item()\n",
    "    accuracy = corrects / targets.size(0)\n",
    "    return accuracy\n",
    "\n",
    "def train_model(config):\n",
    "\n",
    "    model = import_model(config)\n",
    "\n",
    "    (x_train, y_train), (x_val, y_val), (x_test, y_test) = load_and_split_data(config)\n",
    "\n",
    "    train_info = config[\"Train_info\"]\n",
    "\n",
    "    train_dataset = TensorDataset(torch.from_numpy(x_train).float(), torch.from_numpy(y_train).long())\n",
    "    val_dataset = TensorDataset(torch.from_numpy(x_val).float(), torch.from_numpy(y_val).long())\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=train_info[\"batch\"], shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=train_info[\"batch\"], shuffle=False)\n",
    "\n",
    "    optimizer = get_optimizer(train_info[\"optimizer\"], model.parameters(), train_info[\"learning_rate\"])\n",
    "    loss_fn = get_loss(train_info[\"loss\"])\n",
    "\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "    train_acc_history = []\n",
    "    val_acc_history = []\n",
    "    best_val_loss = float('inf')\n",
    "    best_model = None\n",
    "    \n",
    "    for epoch in range(1, train_info[\"epoch\"]+1):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        \n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            running_corrects += (torch.max(outputs, 1)[1] == targets).sum().item()\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_acc = running_corrects / len(train_loader.dataset)\n",
    "        train_loss_history.append(epoch_loss)\n",
    "        train_acc_history.append(epoch_acc)\n",
    "        \n",
    "        model.eval()\n",
    "        running_val_loss = 0.0\n",
    "        running_val_corrects = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = loss_fn(outputs, targets)\n",
    "                running_val_loss += loss.item()\n",
    "                running_val_corrects += (torch.max(outputs, 1)[1] == targets).sum().item()\n",
    "        \n",
    "        val_loss = running_val_loss / len(val_loader)\n",
    "        val_acc = running_val_corrects / len(val_loader.dataset)\n",
    "        val_loss_history.append(val_loss)\n",
    "        val_acc_history.append(val_acc)\n",
    "        \n",
    "        print(f'Epoch [{epoch}/{train_info[\"epoch\"]}], Train Acc: {epoch_acc:.4f}, Val Acc: {val_acc:.4f}, Train Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "        \n",
    "        # 最良モデルの保存\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model = model.state_dict()\n",
    "\n",
    "    # 最良モデルの保存\n",
    "    user_id = config[\"user_id\"]\n",
    "    project_name = config[\"Project_name\"]\n",
    "    model_id = config[\"model_id\"]\n",
    "    base_dir = os.path.abspath(os.path.join(os.getcwd(), \"../user\", user_id, project_name, model_id))\n",
    "    os.makedirs(base_dir, exist_ok=True)\n",
    "    best_model_path = os.path.join(base_dir, \"best_model.pth\")\n",
    "    torch.save(best_model, best_model_path)\n",
    "    \n",
    "    # 学習曲線の保存\n",
    "    photo_dir = os.path.join(base_dir, \"photo\")\n",
    "    os.makedirs(photo_dir, exist_ok=True)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title(\"Training Accuracy\")\n",
    "    plt.plot(range(1, train_info[\"epoch\"]+1), train_acc_history, label=\"Train Accuracy\")\n",
    "    plt.plot(range(1, train_info[\"epoch\"]+1), val_acc_history, label=\"Validation Accuracy\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(photo_dir, \"accuracy_curve.png\"))\n",
    "    plt.close()\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title('Training Loss')\n",
    "    plt.plot(range(1, train_info[\"epoch\"]+1), train_loss_history, label=\"Train Loss\")\n",
    "    plt.plot(range(1, train_info[\"epoch\"]+1), val_loss_history, label=\"Validation Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(photo_dir, \"loss_curve.png\"))\n",
    "    plt.close()\n",
    "\n",
    "# 実行\n",
    "config = {\n",
    "    \"user_id\": \"example_user_id\",\n",
    "    \"Project_name\": \"CIFAR-10\",\n",
    "    \"model_id\": \"example_model\",\n",
    "    \"Train_info\": {\n",
    "        \"Pretreatment\": \"none\",\n",
    "        \"loss\": \"cross_entropy\",\n",
    "        \"optimizer\": \"Adam\",\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"batch\": 16,\n",
    "        \"epoch\": 2,\n",
    "        \"test_size\": 0.2\n",
    "    }\n",
    "}\n",
    "\n",
    "train_model(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Train Acc: 0.2576, Val Acc: 0.3055, Train Loss: 1.9973, Val Loss: 1.8746\n",
      "Epoch [2/2], Train Acc: 0.3152, Val Acc: 0.3419, Train Loss: 1.8629, Val Loss: 1.7804\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import importlib.util\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# GCN関数\n",
    "def global_contrast_normalization(X, scale=1.0, min_divisor=1e-8):\n",
    "    mean = X.mean(axis=1, keepdims=True)\n",
    "    X = X - mean\n",
    "    contrast = np.sqrt((X**2).sum(axis=1, keepdims=True))\n",
    "    contrast[contrast < min_divisor] = min_divisor\n",
    "    X = scale * X / contrast\n",
    "    return X\n",
    "\n",
    "# ZCA Whitening関数\n",
    "def zca_whitening(X):\n",
    "    original_shape = X.shape\n",
    "    X = X.reshape(X.shape[0], -1)\n",
    "    sigma = np.dot(X.T, X) / X.shape[0]\n",
    "    U, S, V = np.linalg.svd(sigma)\n",
    "    epsilon = 1e-5\n",
    "    ZCAMatrix = np.dot(U, np.dot(np.diag(1.0 / np.sqrt(S + epsilon)), U.T))\n",
    "    X = np.dot(X, ZCAMatrix)\n",
    "    return X.reshape(original_shape)\n",
    "\n",
    "# データセットを読み込む関数\n",
    "def import_model(config):\n",
    "    user_id = config[\"user_id\"]\n",
    "    project_name = config[\"Project_name\"]\n",
    "    model_id = config[\"model_id\"]\n",
    "    \n",
    "    base_dir = os.path.abspath(os.path.join(os.getcwd(), \"../user\"))\n",
    "    model_path = os.path.join(base_dir, user_id, project_name, model_id, \"model_config.py\")\n",
    "    spec = importlib.util.spec_from_file_location(\"Simple_NN\", model_path)\n",
    "    model_module = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(model_module)\n",
    "    return model_module.Simple_NN()\n",
    "\n",
    "# データセットの読込み＆前処理を行う関数\n",
    "def load_and_split_data(config):\n",
    "    project_name = config[\"Project_name\"]\n",
    "    dataset_dir = os.path.abspath(os.path.join(os.getcwd(), \"../dataset\", project_name))\n",
    "    \n",
    "    x_train = np.load(os.path.join(dataset_dir, \"x_train.npy\"))\n",
    "    y_train = np.load(os.path.join(dataset_dir, \"y_train.npy\"))\n",
    "    x_test = np.load(os.path.join(dataset_dir, \"x_test.npy\"))\n",
    "    y_test = np.load(os.path.join(dataset_dir, \"y_test.npy\"))\n",
    "\n",
    "    pretreatment = config[\"Train_info\"].get(\"Pretreatment\", \"none\")\n",
    "    if pretreatment == \"GCN\":\n",
    "        x_train = global_contrast_normalization(x_train)\n",
    "        x_test = global_contrast_normalization(x_test)\n",
    "    elif pretreatment == \"ZCA\":\n",
    "        x_train = zca_whitening(x_train)\n",
    "        x_test = zca_whitening(x_test)\n",
    "    \n",
    "    test_size = config[\"Train_info\"][\"test_size\"]\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=test_size)\n",
    "\n",
    "    # チャンネル数と画像サイズを取得\n",
    "    if len(x_train.shape) == 2 and x_train.shape[1] == 784:  # フラット化されたMNISTデータ\n",
    "        channels = 1\n",
    "        height, width = 28, 28\n",
    "        x_train = x_train.reshape(-1, channels, height, width)\n",
    "        x_val = x_val.reshape(-1, channels, height, width)\n",
    "        x_test = x_test.reshape(-1, channels, height, width)\n",
    "    elif len(x_train.shape) == 4:  # カラー画像 (batch_size, height, width, channels)\n",
    "        channels = x_train.shape[-1]\n",
    "        height, width = x_train.shape[1], x_train.shape[2]\n",
    "        x_train = x_train.transpose(0, 3, 1, 2)\n",
    "        x_val = x_val.transpose(0, 3, 1, 2)\n",
    "        x_test = x_test.transpose(0, 3, 1, 2)\n",
    "    \n",
    "    return (x_train, y_train), (x_val, y_val), (x_test, y_test)\n",
    "\n",
    "def get_optimizer(optimizer_name, params, lr):\n",
    "    if optimizer_name == 'SGD':\n",
    "        return optim.SGD(params, lr)\n",
    "    if optimizer_name == 'momentum':\n",
    "        return optim.SGD(params, lr, momentum=0.8)\n",
    "    if optimizer_name == 'Adam':\n",
    "        return optim.Adam(params, lr)\n",
    "    if optimizer_name == 'Adagrad':\n",
    "        return optim.Adagrad(params, lr)\n",
    "    if optimizer_name == 'RMSProp':\n",
    "        return optim.RMSprop(params, lr)\n",
    "    if optimizer_name == 'Adadelta':\n",
    "        return optim.Adadelta(params, lr)\n",
    "\n",
    "def get_loss(loss_name):\n",
    "    if loss_name == 'mse_loss':\n",
    "        return nn.MSELoss()\n",
    "    if loss_name == 'cross_entropy':\n",
    "        return nn.CrossEntropyLoss()\n",
    "    if loss_name == 'binary_cross_entropy':\n",
    "        return nn.BCELoss()\n",
    "    if loss_name == 'nll_loss':\n",
    "        return nn.NLLLoss()\n",
    "    if loss_name == 'hinge_embedding_loss':\n",
    "        return nn.HingeEmbeddingLoss()\n",
    "\n",
    "# 正答率を算出する関数\n",
    "def calculate_accuracy(outputs, targets):\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    corrects = (preds == targets).sum().item()\n",
    "    accuracy = corrects / targets.size(0)\n",
    "    return accuracy\n",
    "\n",
    "def train_model(config):\n",
    "    model = import_model(config)\n",
    "\n",
    "    (x_train, y_train), (x_val, y_val), (x_test, y_test) = load_and_split_data(config)\n",
    "\n",
    "    train_info = config[\"Train_info\"]\n",
    "\n",
    "    train_dataset = TensorDataset(torch.from_numpy(x_train).float(), torch.from_numpy(y_train).long())\n",
    "    val_dataset = TensorDataset(torch.from_numpy(x_val).float(), torch.from_numpy(y_val).long())\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=train_info[\"batch\"], shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=train_info[\"batch\"], shuffle=False)\n",
    "\n",
    "    optimizer = get_optimizer(train_info[\"optimizer\"], model.parameters(), train_info[\"learning_rate\"])\n",
    "    loss_fn = get_loss(train_info[\"loss\"])\n",
    "\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "    train_acc_history = []\n",
    "    val_acc_history = []\n",
    "    best_val_loss = float('inf')\n",
    "    best_model = None\n",
    "    \n",
    "    for epoch in range(1, train_info[\"epoch\"]+1):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        \n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            running_corrects += (torch.max(outputs, 1)[1] == targets).sum().item()\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_acc = running_corrects / len(train_loader.dataset)\n",
    "        train_loss_history.append(epoch_loss)\n",
    "        train_acc_history.append(epoch_acc)\n",
    "        \n",
    "        model.eval()\n",
    "        running_val_loss = 0.0\n",
    "        running_val_corrects = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = loss_fn(outputs, targets)\n",
    "                running_val_loss += loss.item()\n",
    "                running_val_corrects += (torch.max(outputs, 1)[1] == targets).sum().item()\n",
    "        \n",
    "        val_loss = running_val_loss / len(val_loader)\n",
    "        val_acc = running_val_corrects / len(val_loader.dataset)\n",
    "        val_loss_history.append(val_loss)\n",
    "        val_acc_history.append(val_acc)\n",
    "        \n",
    "        print(f'Epoch [{epoch}/{train_info[\"epoch\"]}], Train Acc: {epoch_acc:.4f}, Val Acc: {val_acc:.4f}, Train Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model = model.state_dict()\n",
    "\n",
    "    user_id = config[\"user_id\"]\n",
    "    project_name = config[\"Project_name\"]\n",
    "    model_id = config[\"model_id\"]\n",
    "    base_dir = os.path.abspath(os.path.join(os.getcwd(), \"../user\", user_id, project_name, model_id))\n",
    "    os.makedirs(base_dir, exist_ok=True)\n",
    "    best_model_path = os.path.join(base_dir, \"best_model.pth\")\n",
    "    torch.save(best_model, best_model_path)\n",
    "    \n",
    "    photo_dir = os.path.join(base_dir, \"photo\")\n",
    "    os.makedirs(photo_dir, exist_ok=True)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title(\"Training Accuracy\")\n",
    "    plt.plot(range(1, train_info[\"epoch\"]+1), train_acc_history, label=\"Train Accuracy\")\n",
    "    plt.plot(range(1, train_info[\"epoch\"]+1), val_acc_history, label=\"Validation Accuracy\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(photo_dir, \"accuracy_curve.png\"))\n",
    "    plt.close()\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title('Training Loss')\n",
    "    plt.plot(range(1, train_info[\"epoch\"]+1), train_loss_history, label=\"Train Loss\")\n",
    "    plt.plot(range(1, train_info[\"epoch\"]+1), val_loss_history, label=\"Validation Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(photo_dir, \"loss_curve.png\"))\n",
    "    plt.close()\n",
    "\n",
    "# 実行\n",
    "config = {\n",
    "    \"user_id\": \"example_user_id\",\n",
    "    \"Project_name\": \"CIFAR-10\",\n",
    "    \"model_id\": \"example_model\",\n",
    "    \"Train_info\": {\n",
    "        \"Pretreatment\": \"none\",\n",
    "        \"loss\": \"cross_entropy\",\n",
    "        \"optimizer\": \"Adam\",\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"batch\": 16,\n",
    "        \"epoch\": 2,\n",
    "        \"test_size\": 0.2\n",
    "    }\n",
    "}\n",
    "\n",
    "train_model(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
