{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Unexpected type <class 'numpy.ndarray'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 271\u001b[0m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;66;03m# 実行\u001b[39;00m\n\u001b[0;32m    254\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexample_user_id\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    256\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProject_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCIFAR10\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    268\u001b[0m     }\n\u001b[0;32m    269\u001b[0m }\n\u001b[1;32m--> 271\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 186\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m    183\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m    184\u001b[0m running_corrects \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 186\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, targets \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m    187\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m    188\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n",
      "File \u001b[1;32mc:\\Users\\yukihito\\Python_Venv\\DLab\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\yukihito\\Python_Venv\\DLab\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\yukihito\\Python_Venv\\DLab\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\yukihito\\Python_Venv\\DLab\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[7], line 62\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m---> 62\u001b[0m     x_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m     t_train \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt_train[idx], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x_train, t_train\n",
      "File \u001b[1;32mc:\\Users\\yukihito\\Python_Venv\\DLab\\lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\Users\\yukihito\\Python_Venv\\DLab\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\yukihito\\Python_Venv\\DLab\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\yukihito\\Python_Venv\\DLab\\lib\\site-packages\\torchvision\\transforms\\transforms.py:354\u001b[0m, in \u001b[0;36mResize.forward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m    347\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    349\u001b[0m \u001b[38;5;124;03m        img (PIL Image or Tensor): Image to be scaled.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;124;03m        PIL Image or Tensor: Rescaled image.\u001b[39;00m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 354\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mantialias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\yukihito\\Python_Venv\\DLab\\lib\\site-packages\\torchvision\\transforms\\functional.py:465\u001b[0m, in \u001b[0;36mresize\u001b[1;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[0;32m    459\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m max_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(size) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    460\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    461\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_size should only be passed if size specifies the length of the smaller edge, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    462\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi.e. size should be an int or a sequence of length 1 in torchscript mode.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    463\u001b[0m         )\n\u001b[1;32m--> 465\u001b[0m _, image_height, image_width \u001b[38;5;241m=\u001b[39m \u001b[43mget_dimensions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(size, \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m    467\u001b[0m     size \u001b[38;5;241m=\u001b[39m [size]\n",
      "File \u001b[1;32mc:\\Users\\yukihito\\Python_Venv\\DLab\\lib\\site-packages\\torchvision\\transforms\\functional.py:80\u001b[0m, in \u001b[0;36mget_dimensions\u001b[1;34m(img)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F_t\u001b[38;5;241m.\u001b[39mget_dimensions(img)\n\u001b[1;32m---> 80\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_pil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_dimensions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\yukihito\\Python_Venv\\DLab\\lib\\site-packages\\torchvision\\transforms\\_functional_pil.py:31\u001b[0m, in \u001b[0;36mget_dimensions\u001b[1;34m(img)\u001b[0m\n\u001b[0;32m     29\u001b[0m     width, height \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39msize\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [channels, height, width]\n\u001b[1;32m---> 31\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(img)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: Unexpected type <class 'numpy.ndarray'>"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import importlib.util\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "# GCN関数\n",
    "def gcn(x):\n",
    "    mean = np.mean(x, axis=(1,2,3), keepdims=True)\n",
    "    std = np.std(x, axis=(1,2,3), keepdims=True)\n",
    "    return (x - mean) / (std + 1.E-6)\n",
    "\n",
    "# 新しいZCA Whiteningクラス\n",
    "class ZCA_Whitening:\n",
    "    def __init__(self, epsilon=1E-6):\n",
    "        self.epsilon = epsilon\n",
    "        self.mean = None\n",
    "        self.PCA_mat = None\n",
    "        \n",
    "    def fit(self, x):\n",
    "        x = x.astype(np.float64)  # データをfloat64に変換\n",
    "        x = x.reshape(x.shape[0],-1)\n",
    "        self.mean = np.mean(x, axis=0)\n",
    "        x -= self.mean\n",
    "        cov_mat = np.dot(x.T, x) / x.shape[0]\n",
    "        A, L, _ = np.linalg.svd(cov_mat)\n",
    "        self.ZCA_mat = np.dot(A, np.dot(np.diag(1. / (np.sqrt(L) + self.epsilon)), A.T))\n",
    "            \n",
    "    def transform(self, x):\n",
    "        shape = x.shape\n",
    "        x = x.astype(np.float64)  # データをfloat64に変換\n",
    "        x = x.reshape(x.shape[0],-1)\n",
    "        x -= self.mean\n",
    "        x = np.dot(x, self.ZCA_mat)\n",
    "        return x.reshape(shape)\n",
    "\n",
    "\n",
    "# カスタムデータセット\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, x_train, t_train, transform=None):\n",
    "        # data = x_train.astype('float32')\n",
    "        # self.x_train = data\n",
    "        data = np.transpose(x_train, (0, 2, 3, 1)).astype('float32')\n",
    "        self.x_train = []\n",
    "        for i in range(data.shape[0]):\n",
    "            self.x_train.append(Image.fromarray(np.uint8(data[i])))\n",
    "        self.t_train = t_train\n",
    "        if transform is None:\n",
    "            self.transform = transforms.ToTensor()\n",
    "        else:\n",
    "            self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_train)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x_train = self.transform(self.x_train[idx])\n",
    "        t_train = torch.tensor(self.t_train[idx], dtype=torch.long)\n",
    "\n",
    "        return x_train, t_train\n",
    "\n",
    "\n",
    "# モデルをインポートする関数\n",
    "def import_model(config):\n",
    "    user_id = config[\"user_id\"]\n",
    "    project_name = config[\"Project_name\"]\n",
    "    model_id = config[\"model_id\"]\n",
    "    \n",
    "    base_dir = os.path.abspath(os.path.join(os.getcwd(), \"../user\"))\n",
    "    model_path = os.path.join(base_dir, user_id, project_name, model_id, \"model_config.py\")\n",
    "    spec = importlib.util.spec_from_file_location(\"Simple_NN\", model_path)\n",
    "    model_module = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(model_module)\n",
    "    return model_module.Simple_NN()\n",
    "\n",
    "# データセットの読込み＆前処理を行う関数\n",
    "def load_and_split_data(config):\n",
    "    project_name = config[\"Project_name\"]\n",
    "    dataset_dir = os.path.abspath(os.path.join(os.getcwd(), \"../dataset\", project_name))\n",
    "    \n",
    "    x_train = np.load(os.path.join(dataset_dir, \"x_train.npy\"))\n",
    "    y_train = np.load(os.path.join(dataset_dir, \"y_train.npy\"))\n",
    "    x_test = np.load(os.path.join(dataset_dir, \"x_test.npy\"))\n",
    "    y_test = np.load(os.path.join(dataset_dir, \"y_test.npy\"))\n",
    "\n",
    "    pretreatment = config[\"Train_info\"].get(\"Pretreatment\", \"none\")\n",
    "    if pretreatment == \"GCN\":\n",
    "        x_train = gcn(x_train)\n",
    "        x_test = gcn(x_test)\n",
    "    elif pretreatment == \"ZCA\":\n",
    "        zca = ZCA_Whitening()\n",
    "        zca.fit(x_train)\n",
    "        x_train = zca.transform(x_train)\n",
    "        x_test = zca.transform(x_test)\n",
    "    \n",
    "    test_size = config[\"Train_info\"][\"test_size\"]\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=test_size)\n",
    "\n",
    "    image_shape = config[\"Train_info\"][\"image_shape\"]\n",
    "    if len(x_train.shape) == 2 and x_train.shape[1] == image_shape * image_shape:\n",
    "        channels = 1\n",
    "        height, width = image_shape, image_shape\n",
    "        x_train = x_train.reshape(-1, channels, height, width)\n",
    "        x_val = x_val.reshape(-1, channels, height, width)\n",
    "        x_test = x_test.reshape(-1, channels, height, width)\n",
    "    elif len(x_train.shape) == 4:\n",
    "        channels = x_train.shape[-1]\n",
    "        height, width = x_train.shape[1], x_train.shape[2]\n",
    "        x_train = x_train.transpose(0, 3, 1, 2)\n",
    "        x_val = x_val.transpose(0, 3, 1, 2)\n",
    "        x_test = x_test.transpose(0, 3, 1, 2)\n",
    "    \n",
    "    return (x_train, y_train), (x_val, y_val), (x_test, y_test)\n",
    "\n",
    "# オプティマイザを取得する関数\n",
    "def get_optimizer(optimizer_name, params, lr):\n",
    "    if optimizer_name == 'SGD':\n",
    "        return optim.SGD(params, lr)\n",
    "    if optimizer_name == 'momentum':\n",
    "        return optim.SGD(params, lr, momentum=0.8)\n",
    "    if optimizer_name == 'Adam':\n",
    "        return optim.Adam(params, lr)\n",
    "    if optimizer_name == 'Adagrad':\n",
    "        return optim.Adagrad(params, lr)\n",
    "    if optimizer_name == 'RMSProp':\n",
    "        return optim.RMSprop(params, lr)\n",
    "    if optimizer_name == 'Adadelta':\n",
    "        return optim.Adadelta(params, lr)\n",
    "\n",
    "# ロス関数を取得する関数\n",
    "def get_loss(loss_name):\n",
    "    if loss_name == 'mse_loss':\n",
    "        return nn.MSELoss()\n",
    "    if loss_name == 'cross_entropy':\n",
    "        return nn.CrossEntropyLoss()\n",
    "    if loss_name == 'binary_cross_entropy':\n",
    "        return nn.BCELoss()\n",
    "    if loss_name == 'nll_loss':\n",
    "        return nn.NLLLoss()\n",
    "    if loss_name == 'hinge_embedding_loss':\n",
    "        return nn.HingeEmbeddingLoss()\n",
    "\n",
    "# モデルの訓練を行う関数\n",
    "def train_model(config):\n",
    "    model = import_model(config)\n",
    "\n",
    "    (x_train, y_train), (x_val, y_val), (x_test, y_test) = load_and_split_data(config)\n",
    "\n",
    "    train_info = config[\"Train_info\"]\n",
    "\n",
    "    image_shape = config['image_shape']\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((image_shape, image_shape)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    train_dataset = CustomDataset(x_train, y_train, transform)\n",
    "    val_dataset = CustomDataset(x_val, y_val, transform)\n",
    "\n",
    "    # train_dataset = TensorDataset(torch.from_numpy(x_train).float(), torch.from_numpy(y_train).long())\n",
    "    # val_dataset = TensorDataset(torch.from_numpy(x_val).float(), torch.from_numpy(y_val).long())\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=train_info[\"batch\"], shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=train_info[\"batch\"], shuffle=False)\n",
    "\n",
    "    optimizer = get_optimizer(train_info[\"optimizer\"], model.parameters(), train_info[\"learning_rate\"])\n",
    "    loss_fn = get_loss(train_info[\"loss\"])\n",
    "\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "    train_acc_history = []\n",
    "    val_acc_history = []\n",
    "    best_val_loss = float('inf')\n",
    "    best_model = None\n",
    "    \n",
    "    for epoch in range(1, train_info[\"epoch\"]+1):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        \n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            running_corrects += (torch.max(outputs, 1)[1] == targets).sum().item()\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_acc = running_corrects / len(train_loader.dataset)\n",
    "        train_loss_history.append(epoch_loss)\n",
    "        train_acc_history.append(epoch_acc)\n",
    "        \n",
    "        model.eval()\n",
    "        running_val_loss = 0.0\n",
    "        running_val_corrects = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = loss_fn(outputs, targets)\n",
    "                running_val_loss += loss.item()\n",
    "                running_val_corrects += (torch.max(outputs, 1)[1] == targets).sum().item()\n",
    "        \n",
    "        val_loss = running_val_loss / len(val_loader)\n",
    "        val_acc = running_val_corrects / len(val_loader.dataset)\n",
    "        val_loss_history.append(val_loss)\n",
    "        val_acc_history.append(val_acc)\n",
    "        \n",
    "        print(f'Epoch [{epoch}/{train_info[\"epoch\"]}], Train Acc: {epoch_acc:.4f}, Val Acc: {val_acc:.4f}, Train Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model = model.state_dict()\n",
    "\n",
    "    user_id = config[\"user_id\"]\n",
    "    project_name = config[\"Project_name\"]\n",
    "    model_id = config[\"model_id\"]\n",
    "    base_dir = os.path.abspath(os.path.join(os.getcwd(), \"../user\", user_id, project_name, model_id))\n",
    "    os.makedirs(base_dir, exist_ok=True)\n",
    "    best_model_path = os.path.join(base_dir, \"best_model.pth\")\n",
    "    torch.save(best_model, best_model_path)\n",
    "    \n",
    "    photo_dir = os.path.join(base_dir, \"photo\")\n",
    "    os.makedirs(photo_dir, exist_ok=True)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title(\"Training Accuracy\")\n",
    "    plt.plot(range(1, train_info[\"epoch\"]+1), train_acc_history, label=\"Train Accuracy\")\n",
    "    plt.plot(range(1, train_info[\"epoch\"]+1), val_acc_history, label=\"Validation Accuracy\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(photo_dir, \"accuracy_curve.png\"))\n",
    "    plt.close()\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title('Training Loss')\n",
    "    plt.plot(range(1, train_info[\"epoch\"]+1), train_loss_history, label=\"Train Loss\")\n",
    "    plt.plot(range(1, train_info[\"epoch\"]+1), val_loss_history, label=\"Validation Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(photo_dir, \"loss_curve.png\"))\n",
    "    plt.close()\n",
    "\n",
    "# 実行\n",
    "config = {\n",
    "    \"user_id\": \"example_user_id\",\n",
    "    \"Project_name\": \"CIFAR10\",\n",
    "    \"model_id\": \"example_model\",\n",
    "    \"image_shape\": \"30\",\n",
    "    \"Train_info\": {\n",
    "        \"Pretreatment\": \"None\",\n",
    "        \"loss\": \"cross_entropy\",\n",
    "        \"optimizer\": \"Adam\",\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"batch\": 16,\n",
    "        \"epoch\": 10,\n",
    "        \"test_size\": 0.2,\n",
    "        \"image_shape\": 28\n",
    "    }\n",
    "}\n",
    "\n",
    "train_model(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 3, 32, 32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x_train, y_train), (x_val, y_val), (x_test, y_test) = load_and_split_data(config)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m'DLab (Python 3.9.1)' でセルを実行するには、 ipykernel パッケージが必要です。\n",
      "\u001b[1;31m次のコマンドを実行して、'ipykernel' を Python 環境にインストールします。\n",
      "\u001b[1;31mコマンド: 'c:/Users/yukihito/Python_Venv/DLab/Scripts/python.exe -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# デバイスを指定\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# ZCA白色化の実装\n",
    "class ZCAWhitening():\n",
    "    def __init__(self, epsilon=1e-4, device=device):  # 計算が重いのでGPUを用いる\n",
    "        self.epsilon = epsilon\n",
    "        self.device = device\n",
    "\n",
    "    def fit(self, images):  # 変換行列と平均をデータから計算\n",
    "        \"\"\"\n",
    "        Argument\n",
    "        --------\n",
    "        images : numpy.ndarray\n",
    "            入力画像（訓練データ全体）．(N, C, H, W)\n",
    "        \"\"\"\n",
    "        x = torch.tensor(images[0]).reshape(1, -1).to(self.device)  # 画像（1枚）を1次元化\n",
    "        self.mean = torch.zeros([1, x.size()[1]]).to(self.device)  # 平均値を格納するテンソル．xと同じ形状\n",
    "        con_matrix = torch.zeros([x.size()[1], x.size()[1]]).to(self.device)\n",
    "        for i in range(len(images)):  # 各データについての平均を取る\n",
    "            x = torch.tensor(images[i]).reshape(1, -1).to(self.device)\n",
    "            self.mean += x / len(images)\n",
    "            con_matrix += torch.mm(x.t(), x) / len(images)\n",
    "            if i % 10000 == 0:\n",
    "                print(\"{0}/{1}\".format(i, len(images)))\n",
    "        con_matrix -= torch.mm(self.mean.t(), self.mean)\n",
    "        # E: 固有値 V: 固有ベクトルを並べたもの\n",
    "        E, V = torch.linalg.eigh(con_matrix)  # 固有値分解\n",
    "        self.ZCA_matrix = torch.mm(torch.mm(V, torch.diag((E.squeeze()+self.epsilon)**(-0.5))), V.t())  # A(\\Lambda + \\epsilon I)^{1/2}A^T\n",
    "        print(\"completed!\")\n",
    "\n",
    "    def __call__(self, x):\n",
    "        size = x.size()\n",
    "        x = x.reshape(1, -1).to(self.device)\n",
    "        x -= self.mean  # x - \\bar{x}\n",
    "        x = torch.mm(x, self.ZCA_matrix.t())\n",
    "        x = x.reshape(tuple(size))\n",
    "        x = x.to(\"cpu\")\n",
    "        return x\n",
    "\n",
    "# データセットから画像を取得する関数\n",
    "def load_image_from_project(config):\n",
    "    project_name = config[\"Project_name\"]\n",
    "    dataset_dir = os.path.abspath(os.path.join(os.getcwd(), \"../dataset\", project_name))\n",
    "    x_train = np.load(os.path.join(dataset_dir, \"x_train.npy\"))\n",
    "    # ここでは1枚の画像を取得する\n",
    "    return x_train[0]\n",
    "\n",
    "# データセットをロードする関数\n",
    "def load_dataset_from_project(config):\n",
    "    project_name = config[\"Project_name\"]\n",
    "    dataset_dir = os.path.abspath(os.path.join(os.getcwd(), \"../dataset\", project_name))\n",
    "    x_train = np.load(os.path.join(dataset_dir, \"x_train.npy\"))\n",
    "    y_train = np.load(os.path.join(dataset_dir, \"y_train.npy\"))\n",
    "    x_test = np.load(os.path.join(dataset_dir, \"x_test.npy\"))\n",
    "    y_test = np.load(os.path.join(dataset_dir, \"y_test.npy\"))\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "# 画像を表示する関数\n",
    "def show_image(img, title):\n",
    "    plt.imshow(img)\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# GCNの実装（関数として定義）\n",
    "def global_contrast_normalization(image, scale=1., epsilon=1e-8):\n",
    "    \"\"\"\n",
    "    グローバルコントラスト正規化 (GCN) の実装\n",
    "    \"\"\"\n",
    "    mean = np.mean(image)\n",
    "    std = np.sqrt(np.var(image) + epsilon)\n",
    "    return scale * (image - mean) / std\n",
    "\n",
    "# メイン処理\n",
    "if __name__ == \"__main__\":\n",
    "    config = {\n",
    "        \"Project_name\": \"CIFAR-10\"\n",
    "    }\n",
    "\n",
    "    # データセットをロード\n",
    "    x_train, y_train, x_test, y_test = load_dataset_from_project(config)\n",
    "\n",
    "    # データセットから1枚の画像を取得\n",
    "    sample_image = x_train[0]\n",
    "\n",
    "    # 画像が3次元配列であることを確認\n",
    "    if len(sample_image.shape) == 3:  # (H, W, C) の場合\n",
    "        sample_image = sample_image.reshape(1, *sample_image.shape)  # (1, H, W, C) に変換\n",
    "\n",
    "    # ZCAホワイトニングを適用するために準備\n",
    "    zca = ZCAWhitening(device=device)\n",
    "    zca.fit(x_train)\n",
    "\n",
    "    # 画像をテンソルに変換\n",
    "    sample_image_tensor = torch.tensor(sample_image).permute(0, 3, 1, 2).float()  # (1, C, H, W)\n",
    "\n",
    "    # GCNを適用\n",
    "    gcn_image = global_contrast_normalization(sample_image.copy())\n",
    "    gcn_image = gcn_image.reshape(32, 32, 3)  # 画像の形状に戻す\n",
    "\n",
    "    # ZCAホワイトニングを適用\n",
    "    zca_image = zca(sample_image_tensor).permute(0, 2, 3, 1).numpy()  # (1, H, W, C)\n",
    "    zca_image = zca_image.reshape(32, 32, 3)  # 画像の形状に戻す\n",
    "\n",
    "    # 元の画像を表示\n",
    "    original_image = sample_image.reshape(32, 32, 3)\n",
    "    show_image(original_image, \"Original Image\")\n",
    "\n",
    "    # GCN画像を表示\n",
    "    show_image(gcn_image, \"GCN Image\")\n",
    "\n",
    "    # ZCAホワイトニング画像を表示\n",
    "    show_image(zca_image, \"ZCA Whitening Image\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImageClassification:cuda\n",
      "Epoch [1/10], Train Acc: 0.2597, Val Acc: 0.3223, Train Loss: 1.9971, Val Loss: 1.8501\n",
      "Epoch [2/10], Train Acc: 0.3170, Val Acc: 0.3213, Train Loss: 1.8575, Val Loss: 1.8496\n",
      "Epoch [3/10], Train Acc: 0.3393, Val Acc: 0.3366, Train Loss: 1.7977, Val Loss: 1.8393\n",
      "Epoch [4/10], Train Acc: 0.3505, Val Acc: 0.3173, Train Loss: 1.7655, Val Loss: 1.8451\n",
      "Epoch [5/10], Train Acc: 0.3605, Val Acc: 0.3594, Train Loss: 1.7347, Val Loss: 1.7686\n",
      "Epoch [6/10], Train Acc: 0.3715, Val Acc: 0.3550, Train Loss: 1.7072, Val Loss: 1.7882\n",
      "Epoch [7/10], Train Acc: 0.3818, Val Acc: 0.3683, Train Loss: 1.6861, Val Loss: 1.7411\n",
      "Epoch [8/10], Train Acc: 0.3912, Val Acc: 0.3772, Train Loss: 1.6636, Val Loss: 1.6963\n",
      "Epoch [9/10], Train Acc: 0.3992, Val Acc: 0.4044, Train Loss: 1.6380, Val Loss: 1.6331\n",
      "Epoch [10/10], Train Acc: 0.4041, Val Acc: 0.3852, Train Loss: 1.6291, Val Loss: 1.6998\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import importlib.util\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# デバイスの設定\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"ImageClassification:{device}\")\n",
    "\n",
    "# オプティマイザを取得する関数\n",
    "def get_optimizer(optimizer_name, params, lr):\n",
    "    if optimizer_name == 'SGD':\n",
    "        return optim.SGD(params, lr)\n",
    "    if optimizer_name == 'momentum':\n",
    "        return optim.SGD(params, lr, momentum=0.8)\n",
    "    if optimizer_name == 'Adam':\n",
    "        return optim.Adam(params, lr)\n",
    "    if optimizer_name == 'Adagrad':\n",
    "        return optim.Adagrad(params, lr)\n",
    "    if optimizer_name == 'RMSProp':\n",
    "        return optim.RMSprop(params, lr)\n",
    "    if optimizer_name == 'Adadelta':\n",
    "        return optim.Adadelta(params, lr)\n",
    "\n",
    "# ロス関数を取得する関数\n",
    "def get_loss(loss_name):\n",
    "    if loss_name == 'mse_loss':\n",
    "        return nn.MSELoss()\n",
    "    if loss_name == 'cross_entropy':\n",
    "        return nn.CrossEntropyLoss()\n",
    "    if loss_name == 'binary_cross_entropy':\n",
    "        return nn.BCELoss()\n",
    "    if loss_name == 'nll_loss':\n",
    "        return nn.NLLLoss()\n",
    "    if loss_name == 'hinge_embedding_loss':\n",
    "        return nn.HingeEmbeddingLoss()\n",
    "\n",
    "# GCN関数\n",
    "def gcn(x):\n",
    "    mean = np.mean(x, axis=(1,2,3), keepdims=True)\n",
    "    std = np.std(x, axis=(1,2,3), keepdims=True)\n",
    "    return (x - mean) / (std + 1.E-6)\n",
    "\n",
    "# 新しいZCA Whiteningクラス\n",
    "class ZCA_Whitening:\n",
    "    def __init__(self, epsilon=1E-6):\n",
    "        self.epsilon = epsilon\n",
    "        self.mean = None\n",
    "        self.PCA_mat = None\n",
    "        \n",
    "    def fit(self, x):\n",
    "        x = x.astype(np.float64)\n",
    "        x = x.reshape(x.shape[0],-1)\n",
    "        self.mean = np.mean(x, axis=0)\n",
    "        x -= self.mean\n",
    "        cov_mat = np.dot(x.T, x) / x.shape[0]\n",
    "        A, L, _ = np.linalg.svd(cov_mat)\n",
    "        self.ZCA_mat = np.dot(A, np.dot(np.diag(1. / (np.sqrt(L) + self.epsilon)), A.T))\n",
    "            \n",
    "    def transform(self, x):\n",
    "        shape = x.shape\n",
    "        x = x.astype(np.float64)\n",
    "        x = x.reshape(x.shape[0],-1)\n",
    "        x -= self.mean\n",
    "        x = np.dot(x, self.ZCA_mat)\n",
    "        return x.reshape(shape)\n",
    "\n",
    "# モデルをインポートする関数\n",
    "def import_model(config):\n",
    "    user_id = config[\"user_id\"]\n",
    "    project_name = config[\"Project_name\"]\n",
    "    model_id = config[\"model_id\"]\n",
    "    \n",
    "    base_dir = os.path.abspath(os.path.join(os.getcwd(), \"../user\"))\n",
    "    model_path = os.path.join(base_dir, user_id, project_name, model_id, \"model_config.py\")\n",
    "    spec = importlib.util.spec_from_file_location(\"Simple_NN\", model_path)\n",
    "    model_module = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(model_module)\n",
    "    return model_module.Simple_NN()\n",
    "\n",
    "# データセットの読込み＆前処理を行う関数\n",
    "def load_and_split_data(config):\n",
    "    project_name = config[\"Project_name\"]\n",
    "    dataset_dir = os.path.abspath(os.path.join(os.getcwd(), \"../dataset\", project_name))\n",
    "    \n",
    "    x_train = np.load(os.path.join(dataset_dir, \"x_train.npy\"))\n",
    "    y_train = np.load(os.path.join(dataset_dir, \"y_train.npy\"))\n",
    "    x_test = np.load(os.path.join(dataset_dir, \"x_test.npy\"))\n",
    "    y_test = np.load(os.path.join(dataset_dir, \"y_test.npy\"))\n",
    "\n",
    "    pretreatment = config[\"Train_info\"].get(\"Pretreatment\", \"none\")\n",
    "    if pretreatment == \"GCN\":\n",
    "        x_train = gcn(x_train)\n",
    "        x_test = gcn(x_test)\n",
    "    elif pretreatment == \"ZCA\":\n",
    "        zca = ZCA_Whitening()\n",
    "        zca.fit(x_train)\n",
    "        x_train = zca.transform(x_train)\n",
    "        x_test = zca.transform(x_test)\n",
    "    \n",
    "    test_size = config[\"Train_info\"][\"test_size\"]\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=test_size)\n",
    "\n",
    "    image_shape = config[\"Train_info\"][\"image_shape\"]\n",
    "    if len(x_train.shape) == 2 and x_train.shape[1] == image_shape * image_shape:\n",
    "        channels = 1\n",
    "        height, width = image_shape, image_shape\n",
    "        x_train = x_train.reshape(-1, channels, height, width)\n",
    "        x_val = x_val.reshape(-1, channels, height, width)\n",
    "        x_test = x_test.reshape(-1, channels, height, width)\n",
    "    elif len(x_train.shape) == 4:\n",
    "        channels = x_train.shape[-1]\n",
    "        height, width = x_train.shape[1], x_train.shape[2]\n",
    "        x_train = x_train.transpose(0, 3, 1, 2)\n",
    "        x_val = x_val.transpose(0, 3, 1, 2)\n",
    "        x_test = x_test.transpose(0, 3, 1, 2)\n",
    "    \n",
    "    return (x_train, y_train), (x_val, y_val), (x_test, y_test)\n",
    "\n",
    "# モデルの訓練を行う関数\n",
    "def train_model(config):\n",
    "    model = import_model(config).to(device)\n",
    "\n",
    "    (x_train, y_train), (x_val, y_val), (x_test, y_test) = load_and_split_data(config)\n",
    "\n",
    "    train_info = config[\"Train_info\"]\n",
    "\n",
    "    train_dataset = TensorDataset(torch.from_numpy(x_train).float(), torch.from_numpy(y_train).long())\n",
    "    val_dataset = TensorDataset(torch.from_numpy(x_val).float(), torch.from_numpy(y_val).long())\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=train_info[\"batch\"], shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=train_info[\"batch\"], shuffle=False)\n",
    "\n",
    "    optimizer = get_optimizer(train_info[\"optimizer\"], model.parameters(), train_info[\"learning_rate\"])\n",
    "    loss_fn = get_loss(train_info[\"loss\"])\n",
    "\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "    train_acc_history = []\n",
    "    val_acc_history = []\n",
    "    best_val_loss = float('inf')\n",
    "    best_model = None\n",
    "    \n",
    "    for epoch in range(1, train_info[\"epoch\"]+1):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        \n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            running_corrects += (torch.max(outputs, 1)[1] == targets).sum().item()\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_acc = running_corrects / len(train_loader.dataset)\n",
    "        train_loss_history.append(epoch_loss)\n",
    "        train_acc_history.append(epoch_acc)\n",
    "        \n",
    "        model.eval()\n",
    "        running_val_loss = 0.0\n",
    "        running_val_corrects = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = loss_fn(outputs, targets)\n",
    "                running_val_loss += loss.item()\n",
    "                running_val_corrects += (torch.max(outputs, 1)[1] == targets).sum().item()\n",
    "        \n",
    "        val_loss = running_val_loss / len(val_loader)\n",
    "        val_acc = running_val_corrects / len(val_loader.dataset)\n",
    "        val_loss_history.append(val_loss)\n",
    "        val_acc_history.append(val_acc)\n",
    "        \n",
    "        print(f'Epoch [{epoch}/{train_info[\"epoch\"]}], Train Acc: {epoch_acc:.4f}, Val Acc: {val_acc:.4f}, Train Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model = model.state_dict()\n",
    "\n",
    "    user_id = config[\"user_id\"]\n",
    "    project_name = config[\"Project_name\"]\n",
    "    model_id = config[\"model_id\"]\n",
    "    base_dir = os.path.abspath(os.path.join(os.getcwd(), \"../user\", user_id, project_name, model_id))\n",
    "    os.makedirs(base_dir, exist_ok=True)\n",
    "    best_model_path = os.path.join(base_dir, \"best_model.pth\")\n",
    "    torch.save(best_model, best_model_path)\n",
    "    \n",
    "    photo_dir = os.path.join(base_dir, \"photo\")\n",
    "    os.makedirs(photo_dir, exist_ok=True)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title(\"Training Accuracy\")\n",
    "    plt.plot(range(1, train_info[\"epoch\"]+1), train_acc_history, label=\"Train Accuracy\")\n",
    "    plt.plot(range(1, train_info[\"epoch\"]+1), val_acc_history, label=\"Validation Accuracy\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(photo_dir, \"accuracy_curve.png\"))\n",
    "    plt.close()\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title('Training Loss')\n",
    "    plt.plot(range(1, train_info[\"epoch\"]+1), train_loss_history, label=\"Train Loss\")\n",
    "    plt.plot(range(1, train_info[\"epoch\"]+1), val_loss_history, label=\"Validation Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(photo_dir, \"loss_curve.png\"))\n",
    "    plt.close()\n",
    "\n",
    "config = {\n",
    "    \"user_id\": \"example_user_id\",\n",
    "    \"Project_name\": \"CIFAR10\",\n",
    "    \"model_id\": \"example_model\",\n",
    "    \"Train_info\": {\n",
    "        \"Pretreatment\": \"GCA\",\n",
    "        \"loss\": \"cross_entropy\",\n",
    "        \"optimizer\": \"Adam\",\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"batch\": 16,\n",
    "        \"epoch\": 10,\n",
    "        \"test_size\": 0.2,\n",
    "        \"image_shape\": 28\n",
    "    }\n",
    "}\n",
    "\n",
    "train_model(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
